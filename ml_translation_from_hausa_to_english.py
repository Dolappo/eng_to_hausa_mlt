# -*- coding: utf-8 -*-
"""ML Translation from Hausa to English.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QPGbgGLmH6aYYQWYvxKomaOo7Fy5bIW_
"""

!pip install -q transformers datasets evaluate torch pandas numpy tqdm accelerate

import os
import torch
from transformers import MarianMTModel, MarianTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments
from datasets import load_dataset, Dataset
import evaluate
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import gc
from tqdm import tqdm

gc.enable()

try:
    import torch_xla.core.xla_model as xm
    IS_TPU_AVAILABLE = True
except ImportError:
    IS_TPU_AVAILABLE = False

if IS_TPU_AVAILABLE:
    device = xm.xla_device()
elif torch.cuda.is_available():
    device = torch.device("cuda")
else:
    device = torch.device("cpu")

print(f"Using device: {device}")
print(f"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB") if torch.cuda.is_available() else None

def load_model_and_tokenizer(model_name="Helsinki-NLP/opus-mt-ha-en"):
    """Load model and tokenizer with optimal settings for Colab"""
    tokenizer = MarianTokenizer.from_pretrained(model_name)
    model = MarianMTModel.from_pretrained(
        model_name,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
    )
    return model, tokenizer

def prepare_data(file_path, max_samples=None):
    """Load and prepare data with optional size limit"""
    # For Google Drive
    if file_path.startswith('/content/drive'):
        from google.colab import drive
        drive.mount('/content/drive')

    data = pd.read_csv(file_path)
    if max_samples:
        data = data.sample(n=min(max_samples, len(data)), random_state=42)

    
    data['HAUSA'] = data['HAUSA'].astype(str).str.strip()
    data['ENGLISH'] = data['ENGLISH'].astype(str).str.strip()

    
    data = data[
        (data['HAUSA'].str.len() <= 512) &
        (data['ENGLISH'].str.len() <= 512)
    ]

    return train_test_split(data, test_size=0.2, random_state=42)

def preprocess_function(examples, tokenizer, max_length=128):
    """Preprocess data with proper label padding"""
    inputs = [str(text) for text in examples['HAUSA']]
    targets = [str(text) for text in examples['ENGLISH']]

    
    model_inputs = tokenizer(
        inputs,
        max_length=max_length,
        truncation=True,
        padding="max_length"
    )

    
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            targets,
            max_length=max_length,
            truncation=True,
            padding="max_length"  
        )

    
    labels_with_ignore_index = []
    for label in labels["input_ids"]:
        
        labels_with_ignore_index.append(
            [label_id if label_id != tokenizer.pad_token_id else -100 for label_id in label]
        )

    model_inputs["labels"] = labels_with_ignore_index
    return model_inputs

def compute_metrics(eval_preds):
    """Compute multiple translation metrics"""
    predictions, labels = eval_preds
    decoded_preds = [pred.strip() for pred in tokenizer.batch_decode(predictions, skip_special_tokens=True)]

    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = [[label.strip()] for label in tokenizer.batch_decode(labels, skip_special_tokens=True)]

    metrics = {}

    bleu = evaluate.load("bleu")
    metrics.update(bleu.compute(predictions=decoded_preds, references=decoded_labels))



    return metrics

def main():
    
    if torch.cuda.is_available():
        torch.cuda.empty_cache()


    model, tokenizer = load_model_and_tokenizer()
    model = model.to(device)

   
    train_data, test_data = prepare_data('Eng_Hausa.csv') 

    
    train_dataset = Dataset.from_pandas(train_data)
    test_dataset = Dataset.from_pandas(test_data)


    tokenized_train = train_dataset.map(
        lambda x: preprocess_function(x, tokenizer),
        batched=True,
        batch_size=64,  # Larger batch size for Colab
        remove_columns=train_dataset.column_names
    )

    tokenized_test = test_dataset.map(
        lambda x: preprocess_function(x, tokenizer),
        batched=True,
        batch_size=64,
        remove_columns=test_dataset.column_names
    )

    # Training arguments optimized for Colab
    training_args = Seq2SeqTrainingArguments(
        output_dir="./results",
        evaluation_strategy="epoch",
        save_strategy="epoch",
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        num_train_epochs=10,
        learning_rate=3e-5,
        warmup_ratio=0.1,
        weight_decay=0.01,
        fp16=torch.cuda.is_available(),  
        gradient_accumulation_steps=2,
        logging_dir="./logs",
        logging_steps=100,
        save_total_limit=2,
        load_best_model_at_end=True,
        metric_for_best_model="bleu",
        greater_is_better=True,
        push_to_hub=False,
        gradient_checkpointing=False, 
        optim="adamw_torch",
    )

    # Initialize trainer
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_test,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
    )

    # Train model
    try:
        trainer.train()

        # Save the final model
        trainer.save_model("./final_model")

        # Save to Google Drive if mounted
        if os.path.exists('/content/drive'):
            save_path = '/content/drive/MyDrive/translation_model'
            trainer.save_model(save_path)
            print(f"Model saved to Google Drive at: {save_path}")

        print("Training completed successfully!")

    except Exception as e:
        print(f"An error occurred during training: {str(e)}")

    finally:
        # Cleanup
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()

if __name__ == "__main__":
    main()

